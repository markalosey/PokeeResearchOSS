# Copyright 2025 Pokee AI Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Docker Compose configuration for Pokee Research Agent
# Deploys vLLM server and tool server with GPU support

services:
  vllm-server:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: pokee-vllm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ['0']  # Use GPU 0 only
    shm_size: '1gb'
    ipc: host
    ports:
      - "9999:9999"
    environment:
      - MODEL=${MODEL:-PokeeAI/pokee_research_7b}
      - PORT=9999
      - QUANTIZATION=${QUANTIZATION:-none}
      - HF_TOKEN=${HUGGINGFACE_TOKEN}
      - GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION:-0.65}
      # Model supports up to 32k tokens, but limited by GPU memory
      # Model uses ~14.3 GB on single GPU, leaving ~1 GB free
      # Reduced GPU_MEMORY_UTILIZATION to 0.65 to leave more headroom for profiling
      # Reduced MAX_MODEL_LEN to 6144 to fit in memory
      # Can reduce further to 4096 if still OOM (still 2x improvement over original 2048)
      - MAX_MODEL_LEN=${MAX_MODEL_LEN:-6144}
      - TENSOR_PARALLEL_SIZE=${TENSOR_PARALLEL_SIZE:-1}
      # PyTorch memory optimization to reduce fragmentation
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - vllm-model-cache:/root/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9999/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - pokee-network

  tool-server:
    build:
      context: .
      dockerfile: Dockerfile.tool-server
    container_name: pokee-tool-server
    ports:
      - "8888:8888"
    environment:
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-5-pro}
      - TOOL_SERVER_PORT=8888
    volumes:
      - tool-server-cache:/app/cache
      - tool-server-logs:/app/logs
    depends_on:
      - vllm-server
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - pokee-network

volumes:
  vllm-model-cache:
    driver: local
  tool-server-cache:
    driver: local
  tool-server-logs:
    driver: local

networks:
  pokee-network:
    driver: bridge

