# Copyright 2025 Pokee AI Inc.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Docker Compose configuration for Pokee Research Agent
# Deploys vLLM server and tool server with GPU support

services:
  vllm-server:
    build:
      context: .
      dockerfile: Dockerfile.vllm
    container_name: pokee-vllm
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
              device_ids: ['0']  # Use GPU 0 only
    shm_size: '1gb'
    ipc: host
    ports:
      - "9999:9999"
    environment:
      - MODEL=${MODEL:-PokeeAI/pokee_research_7b}
      - PORT=9999
      - QUANTIZATION=${QUANTIZATION:-none}
      - HF_TOKEN=${HUGGINGFACE_TOKEN}
      - GPU_MEMORY_UTILIZATION=${GPU_MEMORY_UTILIZATION:-0.75}
      # Model supports up to 32k tokens
      # WITHOUT tensor parallelism: Single GPU gets ALL KV cache memory (~8GB available)
      # This allows much larger context windows (16k+) at the cost of slower inference
      # Prioritize context size over speed for research completion
      - MAX_MODEL_LEN=${MAX_MODEL_LEN:-16384}
      - TENSOR_PARALLEL_SIZE=${TENSOR_PARALLEL_SIZE:-1}
      # Remove NCCL vars (not needed for single GPU)
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - vllm-model-cache:/root/.cache/huggingface
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9999/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    networks:
      - pokee-network

  tool-server:
    build:
      context: .
      dockerfile: Dockerfile.tool-server
    container_name: pokee-tool-server
    ports:
      - "8888:8888"
    environment:
      - TAVILY_API_KEY=${TAVILY_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_MODEL=${OPENAI_MODEL:-gpt-5-pro}
      - TOOL_SERVER_PORT=8888
    volumes:
      - tool-server-cache:/app/cache
      - tool-server-logs:/app/logs
    depends_on:
      - vllm-server
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    networks:
      - pokee-network

volumes:
  vllm-model-cache:
    driver: local
  tool-server-cache:
    driver: local
  tool-server-logs:
    driver: local

networks:
  pokee-network:
    driver: bridge

